{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T05:20:50.621420Z",
     "start_time": "2017-11-20T05:20:49.619093Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'konlpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-cdfa35db751f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mdata_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mVocabulary\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdata_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_data_interactive\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Downloads\\Korean_NER_CNN_BiLSTM-master\\data_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# from konlpy.tag import Kkma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# from konlpy.tag import Twitter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkonlpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMecab\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'konlpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import os\n",
    "import argparse\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from data_utils import Vocabulary\n",
    "from data_utils import load_data_interactive\n",
    "\n",
    "from data_loader import prepare_sequence, prepare_char_sequence, prepare_lex_sequence\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from CNN_BiLSTM import CNNBiLSTM\n",
    "from data_loader import get_loader\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T05:20:50.633905Z",
     "start_time": "2017-11-20T05:20:50.623178Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_path='./data_in/vocab_ko_NER.pkl'\n",
    "char_vocab_path='./data_in/char_vocab_ko_NER.pkl'\n",
    "pos_vocab_path='./data_in/pos_vocab_ko_NER.pkl'\n",
    "lex_dict_path='./data_in/lex_dict.pkl'\n",
    "model_load_path='./data_in/cnn_bilstm_tagger-179-400_f1_0.8739_maxf1_0.8739_100_200_2.pkl'\n",
    "num_layers=2\n",
    "embed_size=100\n",
    "hidden_size=200 \n",
    "gpu_index=0\n",
    "\n",
    "predict_NER_dict = {0: '<PAD>',\n",
    "                    1: '<START>',\n",
    "                    2: '<STOP>',\n",
    "                    3: 'B_LC',\n",
    "                    4: 'B_DT',\n",
    "                    5: 'B_OG',\n",
    "                    6: 'B_TI',\n",
    "                    7: 'B_PS',\n",
    "                    8: 'I',\n",
    "                    9: 'O'}\n",
    "\n",
    "NER_idx_dic = {'<unk>': 0, 'LC': 1, 'DT': 2, 'OG': 3, 'TI': 4, 'PS': 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T05:20:50.648788Z",
     "start_time": "2017-11-20T05:20:50.638012Z"
    }
   },
   "outputs": [],
   "source": [
    "def to_np(x):\n",
    "    return x.data.cpu().numpy()\n",
    "\n",
    "def to_var(x, volatile=False):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda(gpu_index)\n",
    "    return Variable(x, volatile=volatile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T05:20:51.914303Z",
     "start_time": "2017-11-20T05:20:51.027562Z"
    }
   },
   "outputs": [],
   "source": [
    "# apply word2vec\n",
    "from gensim.models import word2vec\n",
    "pretrained_word2vec_file = './data_in/word2vec/ko_word2vec_' + str(embed_size) + '.model'\n",
    "wv_model_ko = word2vec.Word2Vec.load(pretrained_word2vec_file)\n",
    "word2vec_matrix = wv_model_ko.wv.syn0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T05:20:51.971027Z",
     "start_time": "2017-11-20T05:20:51.916650Z"
    }
   },
   "outputs": [],
   "source": [
    "# build vocab\n",
    "with open(vocab_path, 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "print(\"len(vocab): \",len(vocab))\n",
    "print(\"word2vec_matrix: \",np.shape(word2vec_matrix))\n",
    "with open(char_vocab_path, 'rb') as f:\n",
    "    char_vocab = pickle.load(f)\n",
    "with open(pos_vocab_path, 'rb') as f:\n",
    "    pos_vocab = pickle.load(f)\n",
    "with open(lex_dict_path, 'rb') as f:\n",
    "    lex_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T05:20:52.700063Z",
     "start_time": "2017-11-20T05:20:52.018458Z"
    }
   },
   "outputs": [],
   "source": [
    "# build models\n",
    "cnn_bilstm_tagger = CNNBiLSTM(vocab_size=len(vocab),\n",
    "                                     char_vocab_size=len(char_vocab),\n",
    "                                        pos_vocab_size=len(pos_vocab),\n",
    "                                        lex_ner_size=len(NER_idx_dic),\n",
    "                                        embed_size=embed_size,\n",
    "                                        hidden_size=hidden_size,\n",
    "                                        num_layers=num_layers,\n",
    "                                        word2vec=word2vec_matrix,\n",
    "                                        num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T05:20:52.834806Z",
     "start_time": "2017-11-20T05:20:52.708097Z"
    }
   },
   "outputs": [],
   "source": [
    "# If you don't use GPU, you can get error here (in the case of loading state dict from Tensor on GPU)\n",
    "#  To avoid error, you should use options -> map_location=lambda storage, loc: storage. it will load tensor to CPU\n",
    "cnn_bilstm_tagger.load_state_dict(torch.load(model_load_path, map_location=lambda storage, loc: storage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T05:20:53.685881Z",
     "start_time": "2017-11-20T05:20:53.563893Z"
    }
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    cnn_bilstm_tagger.cuda(gpu_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T05:20:54.360263Z",
     "start_time": "2017-11-20T05:20:54.342784Z"
    }
   },
   "outputs": [],
   "source": [
    "# inference mode\n",
    "cnn_bilstm_tagger.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T05:20:56.747225Z",
     "start_time": "2017-11-20T05:20:55.514519Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessing(x_text_batch, x_pos_batch, x_split_batch):\n",
    "    x_text_char_item = []\n",
    "    for x_word in x_text_batch[0]:\n",
    "        x_char_item = []\n",
    "        for x_char in x_word:\n",
    "            x_char_item.append(x_char)\n",
    "        x_text_char_item.append(x_char_item)\n",
    "    x_text_char_batch = [x_text_char_item]\n",
    "\n",
    "    x_idx_item = prepare_sequence(x_text_batch[0], vocab.word2idx)\n",
    "    x_idx_char_item = prepare_char_sequence(x_text_char_batch[0], char_vocab.word2idx)\n",
    "    x_pos_item = prepare_sequence(x_pos_batch[0], pos_vocab.word2idx)\n",
    "    x_lex_item = prepare_lex_sequence(x_text_batch[0], lex_dict)\n",
    "\n",
    "    x_idx_batch = [x_idx_item]\n",
    "    x_idx_char_batch = [x_idx_char_item]\n",
    "    x_pos_batch = [x_pos_item]\n",
    "    x_lex_batch = [x_lex_item]\n",
    "\n",
    "\n",
    "    max_word_len = int(np.amax([len(word_tokens) for word_tokens in x_idx_batch])) # ToDo: usually, np.mean can be applied\n",
    "    batch_size = len(x_idx_batch)\n",
    "    batch_words_len = [len(word_tokens) for word_tokens in x_idx_batch]\n",
    "    batch_words_len = np.array(batch_words_len)\n",
    "\n",
    "    # Padding procedure (word)\n",
    "    padded_word_tokens_matrix = np.zeros((batch_size, max_word_len), dtype=np.int64)\n",
    "    for i in range(padded_word_tokens_matrix.shape[0]):\n",
    "        for j in range(padded_word_tokens_matrix.shape[1]):\n",
    "            try:\n",
    "                padded_word_tokens_matrix[i, j] = x_idx_batch[i][j]\n",
    "            except IndexError:\n",
    "                pass\n",
    "\n",
    "    max_char_len = int(np.amax([len(char_tokens) for word_tokens in x_idx_char_batch for char_tokens in word_tokens]))\n",
    "    if max_char_len < 5: # size of maximum filter of CNN\n",
    "        max_char_len = 5\n",
    "        \n",
    "    # Padding procedure (char)\n",
    "    padded_char_tokens_matrix = np.zeros((batch_size, max_word_len, max_char_len), dtype=np.int64)\n",
    "    for i in range(padded_char_tokens_matrix.shape[0]):\n",
    "        for j in range(padded_char_tokens_matrix.shape[1]):\n",
    "            for k in range(padded_char_tokens_matrix.shape[1]):\n",
    "                try:\n",
    "                    padded_char_tokens_matrix[i, j, k] = x_idx_char_batch[i][j][k]\n",
    "                except IndexError:\n",
    "                    pass\n",
    "\n",
    "    # Padding procedure (pos)\n",
    "    padded_pos_tokens_matrix = np.zeros((batch_size, max_word_len), dtype=np.int64)\n",
    "    for i in range(padded_pos_tokens_matrix.shape[0]):\n",
    "        for j in range(padded_pos_tokens_matrix.shape[1]):\n",
    "            try:\n",
    "                padded_pos_tokens_matrix[i, j] = x_pos_batch[i][j]\n",
    "            except IndexError:\n",
    "                pass\n",
    "\n",
    "    # Padding procedure (lex)\n",
    "    padded_lex_tokens_matrix = np.zeros((batch_size, max_word_len, len(NER_idx_dic)))\n",
    "    for i in range(padded_lex_tokens_matrix.shape[0]):\n",
    "        for j in range(padded_lex_tokens_matrix.shape[1]):\n",
    "            for k in range(padded_lex_tokens_matrix.shape[2]):\n",
    "                try:\n",
    "                    for x_lex in x_lex_batch[i][j]:\n",
    "                        k = NER_idx_dic[x_lex]\n",
    "                        padded_lex_tokens_matrix[i, j, k] = 1\n",
    "                except IndexError:\n",
    "                    pass\n",
    "\n",
    "                \n",
    "    x_text_batch = x_text_batch\n",
    "    x_split_batch = x_split_batch\n",
    "    padded_word_tokens_matrix = torch.from_numpy(padded_word_tokens_matrix)\n",
    "    padded_char_tokens_matrix = torch.from_numpy(padded_char_tokens_matrix)\n",
    "    padded_pos_tokens_matrix = torch.from_numpy(padded_pos_tokens_matrix)\n",
    "    padded_lex_tokens_matrix = torch.from_numpy(padded_lex_tokens_matrix).float()\n",
    "    lengths = batch_words_len\n",
    "\n",
    "    return x_text_batch, x_split_batch, padded_word_tokens_matrix, padded_char_tokens_matrix, padded_pos_tokens_matrix, padded_lex_tokens_matrix, lengths\n",
    "\n",
    "def parsing_seq2NER(argmax_predictions, x_text_batch):\n",
    "    predict_NER_list = []\n",
    "    predict_text_NER_result_batch = copy.deepcopy(x_text_batch[0]) #tuple ([],) -> return first list (batch_size == 1)\n",
    "    for argmax_prediction_seq in argmax_predictions:\n",
    "        predict_NER = []\n",
    "        NER_B_flag = None # stop B\n",
    "        prev_NER_token = None\n",
    "        for i, argmax_prediction in enumerate(argmax_prediction_seq):\n",
    "                now_NER_token = predict_NER_dict[argmax_prediction.cpu().data.numpy()[0]]\n",
    "                predict_NER.append(now_NER_token)\n",
    "                if now_NER_token in ['B_LC', 'B_DT', 'B_OG', 'B_TI', 'B_PS'] and NER_B_flag is None: # O B_LC\n",
    "                    NER_B_flag = now_NER_token # start B\n",
    "                    predict_text_NER_result_batch[i] = '<'+predict_text_NER_result_batch[i]\n",
    "                    prev_NER_token = now_NER_token\n",
    "                    if i == len(argmax_prediction_seq)-1:\n",
    "                        predict_text_NER_result_batch[i] = predict_text_NER_result_batch[i]+':'+now_NER_token[-2:]+'>'\n",
    "\n",
    "                elif now_NER_token in ['B_LC', 'B_DT', 'B_OG', 'B_TI', 'B_PS'] and NER_B_flag is not None: # O B_LC B_DT\n",
    "                    predict_text_NER_result_batch[i-1] = predict_text_NER_result_batch[i-1]+':'+prev_NER_token[-2:]+'>'\n",
    "                    predict_text_NER_result_batch[i] = '<' + predict_text_NER_result_batch[i]\n",
    "                    prev_NER_token = now_NER_token\n",
    "                    if i == len(argmax_prediction_seq)-1:\n",
    "                        predict_text_NER_result_batch[i] = predict_text_NER_result_batch[i]+':'+now_NER_token[-2:]+'>'\n",
    "\n",
    "                elif now_NER_token in ['I'] and NER_B_flag is not None:\n",
    "                    if i == len(argmax_prediction_seq) - 1:\n",
    "                        predict_text_NER_result_batch[i] = predict_text_NER_result_batch[i] + ':' + NER_B_flag[-2:] + '>'\n",
    "\n",
    "                elif now_NER_token in ['O'] and NER_B_flag is not None: # O B_LC I O\n",
    "                    predict_text_NER_result_batch[i-1] = predict_text_NER_result_batch[i-1] + ':' + prev_NER_token[-2:] + '>'\n",
    "                    NER_B_flag = None # stop B\n",
    "                    prev_NER_token = now_NER_token\n",
    "\n",
    "        predict_NER_list.append(predict_NER)\n",
    "    return predict_NER_list, predict_text_NER_result_batch\n",
    "\n",
    "def generate_text_result(text_NER_result_batch, x_split_batch):\n",
    "    prev_x_split = 0 \n",
    "    text_string = ''\n",
    "    for i, x_split in enumerate(x_split_batch[0]):\n",
    "        if prev_x_split != x_split:\n",
    "            text_string = text_string+' '+text_NER_result_batch[i]\n",
    "            prev_x_split = x_split\n",
    "        else:\n",
    "            text_string = text_string +''+ text_NER_result_batch[i]\n",
    "            prev_x_split = x_split\n",
    "    return text_string\n",
    "\n",
    "\n",
    "def NER_print(input_str):\n",
    "    input_str.replace(\"  \", \"\")\n",
    "    input_str = input_str.strip()\n",
    "    \n",
    "    x_text_batch, x_pos_batch, x_split_batch = load_data_interactive(input_str)\n",
    "    x_text_batch, x_split_batch, padded_word_tokens_matrix, padded_char_tokens_matrix, padded_pos_tokens_matrix, padded_lex_tokens_matrix, lengths = preprocessing(x_text_batch, x_pos_batch, x_split_batch)\n",
    "    \n",
    "    # Test\n",
    "    argmax_labels_list = []\n",
    "    argmax_predictions_list = []\n",
    "\n",
    "\n",
    "    padded_word_tokens_matrix = to_var(padded_word_tokens_matrix, volatile=True)\n",
    "    padded_char_tokens_matrix = to_var(padded_char_tokens_matrix, volatile=True)\n",
    "    padded_pos_tokens_matrix = to_var(padded_pos_tokens_matrix, volatile=True)\n",
    "    padded_lex_tokens_matrix = to_var(padded_lex_tokens_matrix, volatile=True)\n",
    "\n",
    "\n",
    "    predictions = cnn_bilstm_tagger.sample(padded_word_tokens_matrix, padded_char_tokens_matrix, padded_pos_tokens_matrix, padded_lex_tokens_matrix, lengths)\n",
    "    \n",
    "    max_predictions, argmax_predictions = predictions.max(2)\n",
    "\n",
    "    if len(argmax_predictions.size()) != len(\n",
    "        predictions.size()):  # Check that class dimension is reduced or not (API version issue, pytorch 0.1.12)\n",
    "        max_predictions, argmax_predictions = predictions.max(2, keepdim=True)\n",
    "\n",
    "    argmax_predictions_list.append(argmax_predictions)\n",
    "    \n",
    "    predict_NER_list, predict_text_NER_result_batch = parsing_seq2NER(argmax_predictions, x_text_batch)\n",
    "\n",
    "\n",
    "#     print(\"x_text: \",x_text_batch)\n",
    "#     print(\"NER_pred: \",predict_NER_list)\n",
    "#     print(\"predict_text_NER_result_batch: \",predict_text_NER_result_batch)\n",
    "#     print(\"x_split_batch: \",x_split_batch)\n",
    "    \n",
    "    \n",
    "    origin_text_string = generate_text_result(x_text_batch[0], x_split_batch)\n",
    "    predict_NER_text_string = generate_text_result(predict_text_NER_result_batch, x_split_batch)\n",
    "\n",
    "\n",
    "#     print(\"origin:  \",origin_text_string)\n",
    "#     print(\"predict: \",predict_NER_text_string)\n",
    "    print(\"output> \",predict_NER_text_string)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-20T05:29:12.848154Z",
     "start_time": "2017-11-20T05:26:33.859211Z"
    }
   },
   "outputs": [],
   "source": [
    "# 모델성능 f1 87.39\n",
    "while(True):\n",
    "    \n",
    "    input_str = input('input> ')\n",
    "    \n",
    "    if input_str == 'exit':\n",
    "        break\n",
    "    else:\n",
    "        NER_print(input_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
